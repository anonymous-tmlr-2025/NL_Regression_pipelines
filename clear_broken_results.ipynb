{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/al3615/micromamba/envs/pipeline/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from constants import SEED\n",
    "from pipeline import path_from_config\n",
    "from copy import deepcopy\n",
    "from generate_experiment import hash_experiment_config, recursive_sort\n",
    "from pipeline_components import preprocessers\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def default_seed(experiment: dict, seed: int):\n",
    "    \"\"\"Append seed to experiment if not already present\"\"\"\n",
    "    if \"seed\" not in experiment:\n",
    "        experiment[\"seed\"] = seed\n",
    "    return experiment\n",
    "\n",
    "def extract_preprocessers(component: str):\n",
    "    prefix = \"preprocessers_\"\n",
    "    string_to_split = component[len(prefix):]\n",
    "    valid_preprocessers = [x.name for x in preprocessers]\n",
    "    parsed = []\n",
    "    while string_to_split:\n",
    "        for preprocesser in valid_preprocessers:\n",
    "            if string_to_split.startswith(preprocesser):\n",
    "                parsed.append(preprocesser)\n",
    "                string_to_split = string_to_split[len(preprocesser):]\n",
    "                valid_preprocessers.remove(preprocesser)\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid preprocesser: {string_to_split}\")\n",
    "    return parsed\n",
    "\n",
    "def normalise_preprocessers(path: Path):\n",
    "        prefix = \"preprocessers_\"\n",
    "        new_components = []\n",
    "        for component in path.parts:\n",
    "            if component.startswith(prefix):\n",
    "                preprocessers = extract_preprocessers(component)\n",
    "                sorted_preprocessers = recursive_sort(preprocessers)\n",
    "                sorted_preprocessers_str = \"\".join(sorted_preprocessers)\n",
    "                new_components.append(prefix + sorted_preprocessers_str)\n",
    "            else:\n",
    "                new_components.append(component)\n",
    "        return Path(*new_components)\n",
    "\n",
    "def update_seeds(results_dir: Path):\n",
    "    experiment_log_file = results_dir / \"experiment_log.json\"\n",
    "    with open(experiment_log_file) as f:\n",
    "        experiment_log: dict = json.load(f)\n",
    "    new_log = {}\n",
    "    for experiment in experiment_log.values():\n",
    "        to_hash = experiment.copy()\n",
    "        del to_hash[\"status\"]\n",
    "        del to_hash[\"error\"]\n",
    "        new_hash = hash_experiment_config(to_hash)\n",
    "        experiment = default_seed(experiment, SEED)\n",
    "        experiment[\"preprocesser_order\"] = recursive_sort(experiment[\"preprocesser_order\"])\n",
    "        new_log[new_hash] = experiment\n",
    "    return new_log\n",
    "\n",
    "\n",
    "def clear_results_by_pattern(\n",
    "    results_dir: Path,\n",
    "    preprocessers: list[str] = None,\n",
    "    tokenisers: list[str] = None,\n",
    "    featurisers: list[str] = None,\n",
    "    models: list[str] = None,\n",
    "    backup: bool = True,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Clear entry in experiment log matching specified patterns and optionally backup first.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Directory containing results files\n",
    "        preprocessers: List of preprocesser names to match\n",
    "        tokenisers: List of tokeniser names to match\n",
    "        featurisers: List of featuriser names to match \n",
    "        models: List of model names to match\n",
    "        backup: Whether to backup files before removing\n",
    "    Returns:\n",
    "        New experiment log with entries removed\n",
    "        List of paths to results files to be removed\n",
    "    \"\"\"\n",
    "    logger.info(f\"Clearing results matching patterns in {results_dir}\")\n",
    "    experiment_log_file = results_dir / \"experiment_log.json\"\n",
    "    \n",
    "    if backup:\n",
    "        backup_dir = results_dir / \"backup\"\n",
    "        backup_dir.mkdir(exist_ok=True)\n",
    "        if verbose:\n",
    "            logger.info(f\"Backing up results to {backup_dir}\")\n",
    "        new_path = backup_dir / experiment_log_file.name\n",
    "        if new_path.exists():\n",
    "            raise FileExistsError(f\"Backup file already exists - skipping\")\n",
    "        else:\n",
    "            shutil.copy2(experiment_log_file, new_path)\n",
    "    \n",
    "    removed = 0\n",
    "    with open(experiment_log_file) as f:\n",
    "        experiment_log: dict[str, dict] = json.load(f)\n",
    "    new_log = experiment_log.copy()\n",
    "    paths_to_remove = []\n",
    "    existing_paths = [x for x in results_dir.glob(\"**/results.json\")]\n",
    "    ordered_to_real_map = {}\n",
    "\n",
    "    for path in existing_paths:\n",
    "        normalised_path = normalise_preprocessers(path.parent)\n",
    "        ordered_to_real_map[str(normalised_path)] = path\n",
    "    for hash, experiment in experiment_log.items():\n",
    "        # Check if results match any specified patterns\n",
    "        if preprocessers and not any(p in experiment[\"preprocesser_order\"] for p in preprocessers):\n",
    "            continue\n",
    "        if tokenisers and experiment[\"tokeniser\"] not in tokenisers:\n",
    "            continue\n",
    "        if featurisers and experiment[\"featuriser\"] not in featurisers:\n",
    "            continue\n",
    "        if models and experiment[\"model\"] not in models:\n",
    "            continue\n",
    "            \n",
    "        # If we get here, all specified patterns matched - so we remove it\n",
    "        if verbose:\n",
    "            logger.info(f\"Removing matched experiment: {experiment}\")\n",
    "        del new_log[hash]\n",
    "        candidate_path = path_from_config(\n",
    "                results_dir, experiment[\"preprocesser_order\"], experiment[\"tokeniser\"], \n",
    "                experiment[\"featuriser\"], experiment[\"model\"], \n",
    "                experiment.get(\"seed\", SEED), \n",
    "                experiment.get(\"finetune\", False)\n",
    "        )\n",
    "        normalised_candidate_path = normalise_preprocessers(candidate_path) \n",
    "        if str(normalised_candidate_path) in ordered_to_real_map:\n",
    "            paths_to_remove.append(ordered_to_real_map[str(normalised_candidate_path)])\n",
    "        else:\n",
    "            if verbose:\n",
    "                logger.warning(f\"No matching path found for {candidate_path}\")\n",
    "        removed += 1\n",
    "            \n",
    "    logger.info(f\"Removed {removed} results files matching patterns\")\n",
    "    logger.info(f\"Found {len(paths_to_remove)} paths to remove\")\n",
    "    return new_log, paths_to_remove\n",
    "\n",
    "def update_log(dataset: str, log: dict):\n",
    "    with open(f\"results/{dataset}/experiment_log.json\", \"w\") as f:\n",
    "        json.dump(log, f, indent=0)\n",
    "\n",
    "def delete_paths(paths: list[Path]):\n",
    "    for path in tqdm(paths):\n",
    "        try:\n",
    "            if path.parent.is_dir() and path.parent.exists():\n",
    "                shutil.rmtree(path.parent)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to remove directory: {path.parent} - {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update seeds throughout log\n",
    "datasets = [\"jc_penney_products\", \"online_boat_listings\", \"california_house_prices\"]\n",
    "for dataset in datasets:\n",
    "    clean_log = update_seeds(Path(f\"results/{dataset}\"))\n",
    "    with open(f\"results/{dataset}/experiment_log.json\", \"w\") as f:\n",
    "        json.dump(clean_log, f, indent=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-19 19:54:30.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclear_results_by_pattern\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mClearing results matching patterns in results/california_house_prices\u001b[0m\n",
      "\u001b[32m2024-11-19 19:54:30.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclear_results_by_pattern\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mRemoved 63 results files matching patterns\u001b[0m\n",
      "\u001b[32m2024-11-19 19:54:30.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclear_results_by_pattern\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1mFound 32 paths to remove\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If we want to remove all results with a given model\n",
    "# Might remove MLP as we dont have any activation functions\n",
    "dataset = datasets[2]\n",
    "clean_log, to_delete = clear_results_by_pattern(Path(f\"results/{dataset}\"), models=[\"mlp\", \"resnet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleted 288/288 From JC Penney on 19/11/2024\n",
    "# Deleted 288/288 from online boat listings on 19/11/2024\n",
    "# Deleted 32/63 from california house prices on 19/11/2024 - 31 failures due to OOM -> Use a different GPU/drop batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_log(dataset, clean_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 54.11it/s]\n"
     ]
    }
   ],
   "source": [
    "delete_paths(to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New hash: a61ac505c6c9892e67248db77eef960e9fdb1c1342b57380f381e6bd6ed148f7\n",
      "Old hash: 464ed3feb10b102352769b13bc8eb10d85680b9ea6a71ccc475987636f972e82\n"
     ]
    }
   ],
   "source": [
    "# Debugging duplicates with different hashes\n",
    "\n",
    "sample_config = {\n",
    "    \"preprocesser_order\": [],\n",
    "    \"tokeniser\": \"whitespace\",\n",
    "    \"featuriser\": \"bow_binary\",\n",
    "    \"model\": \"catboost\",\n",
    "    \"seed\": 97,\n",
    "    \"finetune\": False,\n",
    "}\n",
    "\n",
    "new_hash = hash_experiment_config(sample_config)\n",
    "del sample_config[\"seed\"]\n",
    "old_hash = hash_experiment_config(sample_config)\n",
    "print(f\"New hash: {new_hash}\")\n",
    "print(f\"Old hash: {old_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log =update_seeds(Path(f\"results/jc_penney_products\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocesser_order': [],\n",
       " 'tokeniser': 'whitespace',\n",
       " 'featuriser': 'bow_binary',\n",
       " 'model': 'catboost',\n",
       " 'finetune': False,\n",
       " 'seed': 97,\n",
       " 'status': 'success',\n",
       " 'error': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log[new_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'status'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m in_log_not_in_results, in_results_not_in_log, in_both\n\u001b[1;32m     19\u001b[0m current_log \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/jc_penney_products/experiment_log.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread_text())\n\u001b[0;32m---> 20\u001b[0m in_log_not_in_results, in_results_not_in_log, in_both \u001b[38;5;241m=\u001b[39m \u001b[43msync_log_to_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/jc_penney_products\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn log not in results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(in_log_not_in_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn results not in log: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(in_results_not_in_log)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36msync_log_to_results\u001b[0;34m(log, results_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     complete_result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mread_text())\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcomplete_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m complete_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m hash_experiment_config(complete_result)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'status'"
     ]
    }
   ],
   "source": [
    "def sync_log_to_results(log: dict, results_dir: Path):\n",
    "    in_log_not_in_results = {}\n",
    "    in_results_not_in_log = {}\n",
    "    in_both = {}\n",
    "    for result in results_dir.glob(\"**/results.json\"):\n",
    "        complete_result = json.loads(result.read_text())\n",
    "\n",
    "        hash = hash_experiment_config(complete_result)\n",
    "        if hash in log:\n",
    "            in_both[hash] = complete_result\n",
    "        else:\n",
    "            in_results_not_in_log[hash] = complete_result\n",
    "    for hash, experiment in log.items():\n",
    "        if hash not in in_results_not_in_log:\n",
    "            in_log_not_in_results[hash] = experiment\n",
    "    return in_log_not_in_results, in_results_not_in_log, in_both\n",
    "\n",
    "current_log = json.loads(Path(f\"results/jc_penney_products/experiment_log.json\").read_text())\n",
    "in_log_not_in_results, in_results_not_in_log, in_both = sync_log_to_results(current_log, Path(f\"results/jc_penney_products\"))\n",
    "print(f\"In log not in results: {len(in_log_not_in_results)}\")\n",
    "print(f\"In results not in log: {len(in_results_not_in_log)}\")\n",
    "print(f\"In both: {len(in_both)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 260 staged files:\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_staged_files():\n",
    "    # Run git command to list staged files\n",
    "    result = subprocess.run(['git', 'diff', '--name-only', '--cached'], \n",
    "                          capture_output=True, \n",
    "                          text=True)\n",
    "    \n",
    "    # Split output into list of files\n",
    "    staged_files = result.stdout.strip().split('\\n')\n",
    "    \n",
    "    # Filter out empty strings\n",
    "    staged_files = [Path(f) for f in staged_files if f]\n",
    "    \n",
    "    return staged_files\n",
    "\n",
    "staged_files = get_staged_files()\n",
    "print(f\"Found {len(staged_files)} staged files:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:02<00:00, 118.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def delete_parent_dir(path: Path):\n",
    "    if path.parent.is_dir() and path.parent.exists():\n",
    "        shutil.rmtree(path.parent)\n",
    "\n",
    "for file in tqdm(staged_files):\n",
    "    delete_parent_dir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
